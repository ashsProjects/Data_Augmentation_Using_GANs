{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run once to download data\n",
    "# %pip install kaggle\n",
    "# !mkdir -p ~/.kaggle\n",
    "#before running the code below, make sure to download Kaggle API Token first and change path to where the file was downloaded\n",
    "# !mv /path/to/downloaded/kaggle.json ~/.kaggle/\n",
    "\n",
    "# import kaggle\n",
    "# !kaggle datasets download -d nelgiriyewithana/credit-card-fraud-detection-dataset-2023\n",
    "# !kaggle datasets download -d altruistdelhite04/loan-prediction-problem-dataset\n",
    "\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile('credit-card-fraud-detection-dataset-2023.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('credit-card-fraud-detection-dataset')\n",
    "# with zipfile.ZipFile('loan-prediction-problem-dataset.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('loan-prediction-problem-dataset')\n",
    "# !del credit-card-fraud-detection-dataset-2023.zip\n",
    "# !del loan-prediction-problem-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader methods\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "def load_credit_fraud():\n",
    "    #deleting the 1st col as it is just the id; X is 1-30 and y is col 31\n",
    "    X = pd.read_csv('credit-card-fraud-detection-dataset/creditcard_2023.csv', usecols=range(1,31)).values\n",
    "    \n",
    "    #selecting only 5000 of data with labels 0 and 5000 with labels 1\n",
    "    #splitting data by label\n",
    "    indices_0 = X[X[:,-1] == 0]\n",
    "    indices_1 = X[X[:,-1] == 1]\n",
    "    \n",
    "    #randomly selecting indices for labels\n",
    "    random_indices_0 = np.random.choice(indices_0.shape[0], size=5000, replace=False)\n",
    "    random_indices_1 = np.random.choice(indices_1.shape[0], size=5000, replace=False)\n",
    "\n",
    "    #creating 2 subsets with 0 and 1\n",
    "    subset_0 = indices_0[random_indices_0]\n",
    "    subset_1 = indices_1[random_indices_1]\n",
    "\n",
    "    #combining both to create one and shuffling\n",
    "    data = np.vstack((subset_0, subset_1))\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return (data[:,:-1], data[:,-1].astype(int))\n",
    "\n",
    "\n",
    "def load_loan_prediction():\n",
    "    data = pd.read_csv('loan-prediction-problem-dataset/train_u6lujuX_CVtuZ9i.csv', usecols=range(1,13))\n",
    "    data = np.array(data)\n",
    "\n",
    "    y = data[:,-1]\n",
    "    X = data[:,:-1]\n",
    "\n",
    "    y[y=='Y'] = 1\n",
    "    y[y=='N'] = 0\n",
    "\n",
    "    X[X=='Male'] = 1\n",
    "    X[X=='Female'] = 0\n",
    "\n",
    "    X[X=='Yes'] = 1\n",
    "    X[X=='No'] = 0\n",
    "\n",
    "    X[X=='0'] = 0\n",
    "    X[X=='1'] = 1\n",
    "    X[X=='2'] = 2\n",
    "    X[X=='3+'] = 3\n",
    "\n",
    "    X[X=='Graduate'] = 1\n",
    "    X[X=='Not Graduate'] = 0\n",
    "\n",
    "    X[X=='Rural'] = 0\n",
    "    X[X=='Semiurban'] = 1\n",
    "    X[X=='Urban'] = 2\n",
    "\n",
    "    X = X.astype(float)\n",
    "    y = y[~np.isnan(X).any(axis=1)]\n",
    "    X = X[~np.isnan(X).any(axis=1)]\n",
    "    X = X.astype(int)\n",
    "    y = y.astype(int)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "X_loan, y_loan = load_loan_prediction()\n",
    "X_fraud, y_fraud = load_credit_fraud()\n",
    "#Standardizing data\n",
    "X_loan= StandardScaler().fit_transform(X_loan)\n",
    "X_fraud = StandardScaler().fit_transform(X_fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validity for Credit Risk Dataset: {True}\n",
      "Validity for Credit Fraud Dataset: {True}\n"
     ]
    }
   ],
   "source": [
    "#Checking if data is valid\n",
    "def data_is_valid(X,y, examples, features):\n",
    "    return {\n",
    "        X.shape == (examples, features)\n",
    "        and y.shape == (examples,)\n",
    "        and not np.any(np.isnan(X))\n",
    "        and np.all((y==1) | (y==0))\n",
    "    }\n",
    "print(f'Validity for Credit Risk Dataset: {data_is_valid(X_loan, y_loan, 480, 11)}')\n",
    "print(f'Validity for Credit Fraud Dataset: {data_is_valid(X_fraud, y_fraud, 10000, 29)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param grids for classifiers\n",
    "svc_param_grid = {'C': [0.1, 1, 10, 100], 'kernel': ['rbf']}\n",
    "forest_param_grid = {'n_estimators': [50, 100, 150]}\n",
    "\n",
    "#setting cv\n",
    "cv = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell for calculating base accuracies of the credit fraud datatset\n",
    "\n",
    "#svc classifier\n",
    "classifier_1 = GridSearchCV(svm.SVC(), svc_param_grid)\n",
    "classifier_1.fit(X_fraud, y_fraud)\n",
    "fraud_svc_accuracy = np.mean(cross_val_score(classifier_1, X_fraud, y_fraud, cv=cv, scoring='accuracy'))\n",
    "\n",
    "#random forest classifier\n",
    "classifier_2 = GridSearchCV(RandomForestClassifier(), forest_param_grid)\n",
    "classifier_2.fit(X_fraud, y_fraud)\n",
    "fraud_forest_accuracy = np.mean(cross_val_score(classifier_2, X_fraud, y_fraud, cv=cv, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters to use for the fraud dataset (SVC): {'C': 100, 'kernel': 'rbf'}\n",
      "Parameters to use for the fraud dataset (Random Forst): {'n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "print(f'Parameters to use for the fraud dataset (SVC): {classifier_1.best_params_}')\n",
    "print(f'Parameters to use for the fraud dataset (Random Forst): {classifier_2.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell for calculating base accuracies of the loan prediction datatset\n",
    "\n",
    "#svc classifier\n",
    "classifier_1 = GridSearchCV(svm.SVC(), svc_param_grid)\n",
    "classifier_1.fit(X_loan, y_loan)\n",
    "loan_svc_accuracy = np.mean(cross_val_score(classifier_1, X_loan, y_loan, cv=cv, scoring='accuracy'))\n",
    "\n",
    "#random forest classifier\n",
    "classifier_2 = GridSearchCV(RandomForestClassifier(), forest_param_grid)\n",
    "classifier_2.fit(X_loan, y_loan)\n",
    "loan_forest_accuracy = np.mean(cross_val_score(classifier_2, X_loan, y_loan, cv=cv, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters to use for the fraud dataset (SVC): {'C': 1, 'kernel': 'rbf'}\n",
      "Parameters to use for the fraud dataset (Random Forst): {'n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "print(f'Parameters to use for the fraud dataset (SVC): {classifier_1.best_params_}')\n",
    "print(f'Parameters to use for the fraud dataset (Random Forst): {classifier_2.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base accuracies using two classifiers for both datasets:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVC</th>\n",
       "      <th>Random Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Credit Risk Dataset</th>\n",
       "      <td>0.804167</td>\n",
       "      <td>0.797917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Credit Fraud Dataset</th>\n",
       "      <td>0.991000</td>\n",
       "      <td>0.985700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           SVC  Random Forest\n",
       "Credit Risk Dataset   0.804167       0.797917\n",
       "Credit Fraud Dataset  0.991000       0.985700"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_accuracies = [\n",
    "    [loan_svc_accuracy, loan_forest_accuracy],\n",
    "    [fraud_svc_accuracy, fraud_forest_accuracy]\n",
    "]\n",
    "print('Base accuracies using two classifiers for both datasets:')\n",
    "pd.DataFrame(base_accuracies, columns=['SVC', 'Random Forest'], index=['Credit Risk Dataset', 'Credit Fraud Dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries needed for GAN\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Dropout, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#generator and discriminator functions\n",
    "def build_generator(latent_dim, output_dim):\n",
    "    return Sequential([\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(256, input_dim=latent_dim, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "def build_discriminator(input_dim):\n",
    "    return Sequential([\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(256, input_dim=input_dim, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    return Sequential([\n",
    "        generator,\n",
    "        discriminator\n",
    "    ])\n",
    "\n",
    "def train_gan(generator, discriminator, gan, data, latent_dim, epochs, batch_size, verbose):\n",
    "    generator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    gan.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #train discriminator on real data\n",
    "        real_data = data[np.random.randint(0, data.shape[0], batch_size)]\n",
    "        real_labels = np.ones((batch_size, 1))\n",
    "\n",
    "        d_loss_real, d_acc_real = discriminator.train_on_batch(real_data, real_labels)\n",
    "\n",
    "        #train discriminator on generated data\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        generated_data = generator.predict(noise)\n",
    "        fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "        d_loss_fake, d_acc_fake = discriminator.train_on_batch(generated_data, fake_labels)\n",
    "\n",
    "        #train generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        valid_labels = np.ones((batch_size, 1))\n",
    "\n",
    "        g_loss, g_acc = gan.train_on_batch(noise, valid_labels)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Discriminator Real: Epoch {epoch + 1}/{epochs}, Loss: {d_loss_real}, Accuracy: {d_acc_real}\")\n",
    "            print(f\"Discriminator Fake: Epoch {epoch + 1}/{epochs}, Loss: {d_loss_fake}, Accuracy: {d_acc_fake}\")\n",
    "            print(f\"GAN: Epoch {epoch + 1}/{epochs}, Loss: {g_loss}, Accuracy: {g_acc}\")\n",
    "        \n",
    "def generate_synthetic_data(generator, latent_dim, n_samples):\n",
    "    noise = np.random.normal(0, 1, (n_samples, latent_dim))\n",
    "    generated_data = generator.predict(noise)\n",
    "    return generated_data\n",
    "\n",
    "def run_gan(data, num_features, num_noise_vector, n_samples, epochs=200, batch_size=10000, verbose=True):\n",
    "    #scale data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    \n",
    "    #build models\n",
    "    generator = build_generator(num_noise_vector, num_features)\n",
    "    discriminator = build_discriminator(num_features)\n",
    "    gan = build_gan(generator, discriminator)\n",
    "    \n",
    "    #train and generate data using GAN\n",
    "    train_gan(generator, discriminator, gan, scaled_data, num_noise_vector, epochs, batch_size, verbose)\n",
    "    generated_data = generate_synthetic_data(generator, num_noise_vector, n_samples)\n",
    "    \n",
    "    #reverse the scaling on data\n",
    "    generated_data = scaler.inverse_transform(generated_data)\n",
    "    \n",
    "    #return the generated data\n",
    "    return generated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(X,y, classifier_class, parameter):\n",
    "    classifier = classifier_class(kernel=\"rbf\", C=parameter) if classifier_class == svm.SVC else classifier_class(parameter)\n",
    "    classifier.fit(X,y)\n",
    "    accuracy = np.mean(cross_val_score(classifier, X, y, cv=cv, scoring='accuracy'))\n",
    "    return accuracy\n",
    "\n",
    "def train_splits_using_gan(X,y, samples, svc_param):\n",
    "    X_0 = X[y==0]\n",
    "    X_1 = X[y==1]\n",
    "\n",
    "    accuracy_added = []\n",
    "    accuracy_raw = []\n",
    "    splits = [.25,.50,.75,1]\n",
    "\n",
    "    for split in splits:\n",
    "        #splitting and testing X_0\n",
    "        num_samples_0 = int(len(X_0)*split)\n",
    "        num_samples_1 = int(len(X_1)*split)\n",
    "        sampled_data_0 = X_0[:num_samples_0]\n",
    "        sampled_data_1 = X_1[:num_samples_1]\n",
    "        \n",
    "        for sample in samples:\n",
    "            generated_data_0 = run_gan(sampled_data_0, sampled_data_0.shape[1], 10, sample, verbose=False)\n",
    "            generated_data_1 = run_gan(sampled_data_1, sampled_data_1.shape[1], 10, sample, verbose=False)\n",
    "            \n",
    "            combined_generated_data = list(zip(generated_data_0, np.zeros(len(generated_data_0)))) + list(zip(generated_data_1, np.ones(len(generated_data_1))))\n",
    "            np.random.shuffle(combined_generated_data)\n",
    "            X_generated, y_generated = zip(*combined_generated_data)\n",
    "            X_generated = np.array(X_generated)\n",
    "            y_generated = np.array(y_generated)\n",
    "            \n",
    "            #calculate accuracy for raw\n",
    "            kNN_acc = calculate_accuracy(X_generated, y_generated, KNeighborsClassifier)\n",
    "            svc_acc = calculate_accuracy(X_generated, y_generated, svm.SVC, svc_param)#100 for fraud from best_params\n",
    "            accuracy_raw.append([split, sample*2, svc_acc, kNN_acc])\n",
    "            \n",
    "            #calculate accuracy after appending to the original array\n",
    "            combined_X = np.vstack((X,X_generated))\n",
    "            combined_y = np.vstack((y,y_generated))\n",
    "            kNN_acc = calculate_accuracy(combined_X, combined_y, KNeighborsClassifier)\n",
    "            svc_acc = calculate_accuracy(combined_X, combined_y, svm.SVC, svc_param)#100 for fraud from best_params\n",
    "            accuracy_added.append([split, sample*2, (sample*2+len(X)), svc_acc, kNN_acc])\n",
    "            \n",
    "    return (accuracy_added, accuracy_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the accurcies for all splits for both datasets\n",
    "loan_acc_added, loan_acc_raw = train_splits_using_gan(X_loan, y_loan, [50,100,250, len(X_loan//2)], 1)\n",
    "fraud_acc_added, fraud_acc_raw = train_splits_using_gan(X_fraud, y_fraud, [500,1000,2500, len(X_fraud//2)], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the accuracies\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
