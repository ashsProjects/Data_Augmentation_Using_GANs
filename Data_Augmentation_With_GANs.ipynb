{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run once to download data\n",
    "# %pip install kaggle\n",
    "# !mkdir -p ~/.kaggle\n",
    "#before running the code below, make sure to download Kaggle API Token first and change path to where the file was downloaded\n",
    "# !mv /path/to/downloaded/kaggle.json ~/.kaggle/\n",
    "\n",
    "# import kaggle\n",
    "# !kaggle datasets download -d nelgiriyewithana/credit-card-fraud-detection-dataset-2023\n",
    "# !kaggle datasets download -d altruistdelhite04/loan-prediction-problem-dataset\n",
    "\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile('credit-card-fraud-detection-dataset-2023.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('credit-card-fraud-detection-dataset')\n",
    "# with zipfile.ZipFile('loan-prediction-problem-dataset.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('loan-prediction-problem-dataset')\n",
    "# !del credit-card-fraud-detection-dataset-2023.zip\n",
    "# !del credit-risk-classification-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader methods\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def load_credit_risk():\n",
    "    payment = pd.read_csv('credit-risk-classification-dataset/payment_data.csv')\n",
    "    customer = pd.read_csv('credit-risk-classification-dataset/customer_data.csv', usecols=range(0,2))\n",
    "    merged_data = payment.merge(customer, left_on='id', right_on='id').values\n",
    "    \n",
    "    cols_to_drop = [0,8,11]#dropping the id and dates; not helpful\n",
    "    data = np.delete(merged_data, cols_to_drop, axis=1)\n",
    "    \n",
    "    #selecting only 1000 of data with labels 0 and 1000 with labels 1\n",
    "    #splitting data by label\n",
    "    indices_0 = data[data[:,-1] == 0]\n",
    "    indices_1 = data[data[:,-1] == 1]\n",
    "    \n",
    "    #randomly selecting indices for labels\n",
    "    random_indices_0 = np.random.choice(indices_0.shape[0], size=500, replace=False)\n",
    "    random_indices_1 = np.random.choice(indices_1.shape[0], size=500, replace=False)\n",
    "\n",
    "    #creating 2 subsets with 0 and 1\n",
    "    subset_0 = indices_0[random_indices_0]\n",
    "    subset_1 = indices_1[random_indices_1]\n",
    "\n",
    "    #combining both to create one and shuffling\n",
    "    data = np.vstack((subset_0, subset_1))\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    return (data[:,:-1], data[:,-1].astype(int))\n",
    "    \n",
    "def load_credit_fraud():\n",
    "    #deleting the 1st col as it is just the id; X is 1-30 and y is col 31\n",
    "    X = pd.read_csv('credit-card-fraud-detection-dataset/creditcard_2023.csv', usecols=range(1,31)).values\n",
    "    \n",
    "    #selecting only 5000 of data with labels 0 and 5000 with labels 1\n",
    "    #splitting data by label\n",
    "    indices_0 = X[X[:,-1] == 0]\n",
    "    indices_1 = X[X[:,-1] == 1]\n",
    "    \n",
    "    #randomly selecting indices for labels\n",
    "    random_indices_0 = np.random.choice(indices_0.shape[0], size=5000, replace=False)\n",
    "    random_indices_1 = np.random.choice(indices_1.shape[0], size=5000, replace=False)\n",
    "\n",
    "    #creating 2 subsets with 0 and 1\n",
    "    subset_0 = indices_0[random_indices_0]\n",
    "    subset_1 = indices_1[random_indices_1]\n",
    "\n",
    "    #combining both to create one and shuffling\n",
    "    data = np.vstack((subset_0, subset_1))\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return (data[:,:-1], data[:,-1].astype(int))\n",
    "\n",
    "\n",
    "def load_loan_prediction():\n",
    "    data = pd.read_csv('loan-prediction-problem-dataset/train_u6lujuX_CVtuZ9i.csv', usecols=range(1,13))\n",
    "    data = np.array(data)\n",
    "\n",
    "    y = data[:,-1]\n",
    "    X = data[:,:-1]\n",
    "\n",
    "    y[y=='Y'] = 1\n",
    "    y[y=='N'] = 0\n",
    "\n",
    "    X[X=='Male'] = 1\n",
    "    X[X=='Female'] = 0\n",
    "\n",
    "    X[X=='Yes'] = 1\n",
    "    X[X=='No'] = 0\n",
    "\n",
    "    X[X=='0'] = 0\n",
    "    X[X=='1'] = 1\n",
    "    X[X=='2'] = 2\n",
    "    X[X=='3+'] = 3\n",
    "\n",
    "    X[X=='Graduate'] = 1\n",
    "    X[X=='Not Graduate'] = 0\n",
    "\n",
    "    X[X=='Rural'] = 0\n",
    "    X[X=='Semiurban'] = 1\n",
    "    X[X=='Urban'] = 2\n",
    "\n",
    "    X = X.astype(float)\n",
    "    y = y[~np.isnan(X).any(axis=1)]\n",
    "    X = X[~np.isnan(X).any(axis=1)]\n",
    "    X = X.astype(int)\n",
    "    y = y.astype(int)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "X_loan, y_loan = load_loan_prediction()\n",
    "X_fraud, y_fraud = load_credit_fraud()\n",
    "#Standardizing data\n",
    "X_risk = StandardScaler().fit_transform(X_loan)\n",
    "X_fraud = StandardScaler().fit_transform(X_fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if data is valid\n",
    "def data_is_valid(X,y, examples, features):\n",
    "    return {\n",
    "        X.shape == (examples, features)\n",
    "        and y.shape == (examples,)\n",
    "        and not np.any(np.isnan(X))\n",
    "        and np.all((y==1) | (y==0))\n",
    "    }\n",
    "print(f'Validity for Credit Risk Dataset: {data_is_valid(X_risk, y_loan, 480, 11)}')\n",
    "print(f'Validity for Credit Fraud Dataset: {data_is_valid(X_fraud, y_fraud, 10000, 29)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param grids for classifiers\n",
    "svc_param_grid = {'C': [0.1, 1, 10, 100], 'kernel': ['rbf']}\n",
    "kNN_param_grid = {'n_neighbors': [3,5,7,9]}\n",
    "forest_param_grid = {'n_estimators': [50, 100, 150]}\n",
    "\n",
    "#setting cv\n",
    "cv = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell for calculating base accuracies of the credit fraud datatset\n",
    "\n",
    "#svc classifier\n",
    "classifier_1 = GridSearchCV(svm.SVC(), svc_param_grid)\n",
    "classifier_1.fit(X_fraud, y_fraud)\n",
    "fraud_svc_accuracy = np.mean(cross_val_score(classifier_1, X_fraud, y_fraud, cv=cv, scoring='accuracy'))\n",
    "\n",
    "#kNN classifier\n",
    "classifier_2 = GridSearchCV(KNeighborsClassifier(), kNN_param_grid)\n",
    "classifier_2.fit(X_fraud, y_fraud)\n",
    "fraud_kNN_accuracy = np.mean(cross_val_score(classifier_2, X_fraud, y_fraud, cv=cv, scoring='accuracy'))\n",
    "\n",
    "#random forest classifier\n",
    "classifier_3 = GridSearchCV(RandomForestClassifier(), forest_param_grid)\n",
    "classifier_3.fit(X_fraud, y_fraud)\n",
    "fraud_forest_accuracy = np.mean(cross_val_score(classifier_3, X_fraud, y_fraud, cv=cv, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell for calculating base accuracies of the loan prediction datatset\n",
    "\n",
    "#svc classifier\n",
    "classifier_1 = GridSearchCV(svm.SVC(), svc_param_grid)\n",
    "classifier_1.fit(X_loan, y_loan)\n",
    "loan_svc_accuracy = np.mean(cross_val_score(classifier_1, X_loan, y_loan, cv=cv, scoring='accuracy'))\n",
    "\n",
    "#kNN classifier\n",
    "classifier_2 = GridSearchCV(KNeighborsClassifier(), kNN_param_grid)\n",
    "classifier_2.fit(X_loan, y_loan)\n",
    "loan_kNN_accuracy = np.mean(cross_val_score(classifier_2, X_loan, y_loan, cv=cv, scoring='accuracy'))\n",
    "\n",
    "#random forest classifier\n",
    "classifier_3 = GridSearchCV(RandomForestClassifier(), forest_param_grid)\n",
    "classifier_3.fit(X_loan, y_loan)\n",
    "loan_forest_accuracy = np.mean(cross_val_score(classifier_3, X_loan, y_loan, cv=cv, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_accuracies = [\n",
    "    [loan_svc_accuracy, loan_kNN_accuracy, loan_forest_accuracy],\n",
    "    [fraud_svc_accuracy, fraud_kNN_accuracy, fraud_forest_accuracy]\n",
    "]\n",
    "print('Base accuracies using three classifiers for both datasets:')\n",
    "pd.DataFrame(base_accuracies, columns=['SVC', 'kNN', 'Random Forest'], index=['Credit Risk Dataset', 'Credit Fraud Dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries needed for GAN\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Dropout, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#generator and discriminator functions\n",
    "def build_generator(latent_dim, output_dim):\n",
    "    return Sequential([\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(256, input_dim=latent_dim, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "def build_discriminator(input_dim):\n",
    "    return Sequential([\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(256, input_dim=input_dim, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    return Sequential([\n",
    "        generator,\n",
    "        discriminator\n",
    "    ])\n",
    "\n",
    "def train_gan(generator, discriminator, gan, data, latent_dim, epochs, batch_size, verbose):\n",
    "    generator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    gan.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    gan_acc = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #train discriminator on real data\n",
    "        real_data = data[np.random.randint(0, data.shape[0], batch_size)]\n",
    "        real_labels = np.ones((batch_size, 1))\n",
    "\n",
    "        d_loss_real, d_acc_real = discriminator.train_on_batch(real_data, real_labels)\n",
    "\n",
    "        #train discriminator on generated data\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        generated_data = generator.predict(noise)\n",
    "        fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "        d_loss_fake, d_acc_fake = discriminator.train_on_batch(generated_data, fake_labels)\n",
    "\n",
    "        #train generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        valid_labels = np.ones((batch_size, 1))\n",
    "\n",
    "        g_loss, g_acc = gan.train_on_batch(noise, valid_labels)\n",
    "        \n",
    "        gan_acc.append(g_acc)\n",
    "        if verbose == 1:\n",
    "            print(f\"Discriminator Real: Epoch {epoch + 1}/{epochs}, Loss: {d_loss_real}, Accuracy: {d_acc_real}\")\n",
    "            print(f\"Discriminator Fake: Epoch {epoch + 1}/{epochs}, Loss: {d_loss_fake}, Accuracy: {d_acc_fake}\")\n",
    "            print(f\"GAN: Epoch {epoch + 1}/{epochs}, Loss: {g_loss}, Accuracy: {g_acc}\")\n",
    "    return gan_acc\n",
    "        \n",
    "def generate_synthetic_data(generator, latent_dim, n_samples):\n",
    "    noise = np.random.normal(0, 1, (n_samples, latent_dim))\n",
    "    generated_data = generator.predict(noise)\n",
    "    return generated_data\n",
    "\n",
    "def run_gan(data, num_features, num_noise_vector, epochs=300, batch_size=10000, n_samples=500, verbose=1):\n",
    "    #scale data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    \n",
    "    #build models\n",
    "    generator = build_generator(num_noise_vector, num_features)\n",
    "    discriminator = build_discriminator(num_features)\n",
    "    gan = build_gan(generator, discriminator)\n",
    "    \n",
    "    #train and generate data using GAN\n",
    "    gan_acc = train_gan(generator, discriminator, gan, scaled_data, num_noise_vector, epochs, batch_size, verbose)\n",
    "    generated_data = generate_synthetic_data(generator, num_noise_vector, n_samples)\n",
    "    \n",
    "    #reverse the scaling on data\n",
    "    generated_data = scaler.inverse_transform(generated_data)\n",
    "    \n",
    "    #return the generated data\n",
    "    return (generated_data, gan_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "X,y = load_wine(return_X_y=True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data, accuracy = run_gan(X, X.shape[1], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(200), accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
