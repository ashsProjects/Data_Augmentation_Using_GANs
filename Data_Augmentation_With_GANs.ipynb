{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading loan-prediction-problem-dataset.zip to c:\\Users\\tyler\\Dropbox\\School\\CS345\\Data_Augmentation_Using_GANs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/12.6k [00:00<?, ?B/s]\n",
      "100%|██████████| 12.6k/12.6k [00:00<00:00, 1.27MB/s]\n"
     ]
    }
   ],
   "source": [
    "#RUN ONLY ONCE\n",
    "# %pip install kaggle\n",
    "# !mkdir -p ~/.kaggle\n",
    "#before running the code below, make sure to download Kaggle API Token first and change path to where the file was downloaded\n",
    "# !mv /path/to/downloaded/kaggle.json ~/.kaggle/\n",
    "\n",
    "# import kaggle\n",
    "# !kaggle datasets download -d nelgiriyewithana/credit-card-fraud-detection-dataset-2023\n",
    "# !kaggle datasets download -d praveengovi/credit-risk-classification-dataset\n",
    "\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile('credit-card-fraud-detection-dataset-2023.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('credit-card-fraud-detection-dataset')\n",
    "# with zipfile.ZipFile('credit-risk-classification-dataset.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('credit-risk-classification-dataset')\n",
    "# !del credit-card-fraud-detection-dataset-2023.zip\n",
    "# !del credit-risk-classification-dataset.zip\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d altruistdelhite04/loan-prediction-problem-dataset\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile('loan-prediction-problem-dataset.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('loan-prediction-problem-dataset')\n",
    "\n",
    "# !del loan-prediction-problem-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kaggle\n",
    "# !kaggle datasets download -d joebeachcapital/tuandromd\n",
    "\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile('tuandromd.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('malware-dataset')\n",
    "# !del tuandromd.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader methods\n",
    "def load_credit_risk():\n",
    "    payment = pd.read_csv('credit-risk-classification-dataset/payment_data.csv')\n",
    "    customer = pd.read_csv('credit-risk-classification-dataset/customer_data.csv', usecols=range(0,2))\n",
    "    merged_data = payment.merge(customer, left_on='id', right_on='id').values\n",
    "    \n",
    "    cols_to_drop = [0,8,11]#dropping the id and dates; not helpful\n",
    "    data = np.delete(merged_data, cols_to_drop, axis=1)\n",
    "    \n",
    "    #selecting only 1000 of data with labels 0 and 1000 with labels 1\n",
    "    #splitting data by label\n",
    "    indices_0 = data[data[:,-1] == 0]\n",
    "    indices_1 = data[data[:,-1] == 1]\n",
    "    \n",
    "    #randomly selecting indices for labels\n",
    "    random_indices_0 = np.random.choice(indices_0.shape[0], size=1000, replace=False)\n",
    "    random_indices_1 = np.random.choice(indices_1.shape[0], size=1000, replace=False)\n",
    "\n",
    "    #creating 2 subsets with 0 and 1\n",
    "    subset_0 = indices_0[random_indices_0]\n",
    "    subset_1 = indices_1[random_indices_1]\n",
    "\n",
    "    #combining both to create one and shuffling\n",
    "    data = np.vstack((subset_0, subset_1))\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    return (data[:,:-1], data[:,-1].astype(int))\n",
    "    \n",
    "\n",
    "def load_credit_fraud():\n",
    "    #deleting the 1st col as it is just the id; X is 1-30 and y is col 31\n",
    "    X = pd.read_csv('credit-card-fraud-detection-dataset/creditcard_2023.csv', usecols=range(1,31)).values\n",
    "    \n",
    "    #selecting only 5000 of data with labels 0 and 5000 with labels 1\n",
    "    #splitting data by label\n",
    "    indices_0 = X[X[:,-1] == 0]\n",
    "    indices_1 = X[X[:,-1] == 1]\n",
    "    \n",
    "    #randomly selecting indices for labels\n",
    "    random_indices_0 = np.random.choice(indices_0.shape[0], size=5000, replace=False)\n",
    "    random_indices_1 = np.random.choice(indices_1.shape[0], size=5000, replace=False)\n",
    "\n",
    "    #creating 2 subsets with 0 and 1\n",
    "    subset_0 = indices_0[random_indices_0]\n",
    "    subset_1 = indices_1[random_indices_1]\n",
    "\n",
    "    #combining both to create one and shuffling\n",
    "    data = np.vstack((subset_0, subset_1))\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return (data[:,:-1], data[:,-1].astype(int))\n",
    "\n",
    "\n",
    "def load_loan_prediction():\n",
    "    data = pd.read_csv('loan-prediction-problem-dataset/train_u6lujuX_CVtuZ9i.csv', usecols=range(1,13))\n",
    "    data = np.array(data)\n",
    "\n",
    "    y = data[:,-1]\n",
    "    X = data[:,:-1]\n",
    "\n",
    "    y[y=='Y'] = 1\n",
    "    y[y=='N'] = 0\n",
    "\n",
    "    X[X=='Male'] = 1\n",
    "    X[X=='Female'] = 0\n",
    "\n",
    "    X[X=='Yes'] = 1\n",
    "    X[X=='No'] = 0\n",
    "\n",
    "    X[X=='0'] = 0\n",
    "    X[X=='1'] = 1\n",
    "    X[X=='2'] = 2\n",
    "    X[X=='3+'] = 3\n",
    "\n",
    "    X[X=='Graduate'] = 1\n",
    "    X[X=='Not Graduate'] = 0\n",
    "\n",
    "    X[X=='Rural'] = 0\n",
    "    X[X=='Semiurban'] = 1\n",
    "    X[X=='Urban'] = 2\n",
    "\n",
    "    X = X.astype(float)\n",
    "    y = y[~np.isnan(X).any(axis=1)]\n",
    "    X = X[~np.isnan(X).any(axis=1)]\n",
    "    X = X.astype(int)\n",
    "    y = y.astype(int)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def load_malware():\n",
    "    data = np.array(pd.read_csv('malware-dataset/TUANDROMD.csv'))\n",
    "    \n",
    "    indices_0 = data[data[:,-1] == 'malware']\n",
    "    indices_1 = data[data[:,-1] == 'goodware']\n",
    "\n",
    "    random_indices_0 = np.random.choice(indices_0.shape[0], size=800, replace=False)\n",
    "    random_indices_1 = np.random.choice(indices_1.shape[0], size=800, replace=False)\n",
    "\n",
    "    subset_0 = indices_0[random_indices_0]\n",
    "    subset_1 = indices_1[random_indices_1]\n",
    "\n",
    "    data = np.vstack((subset_0, subset_1))\n",
    "    \n",
    "    data[data[:,-1] == 'goodware'] = 0\n",
    "    data[data[:,-1] == 'malware'] = 1\n",
    "\n",
    "    X = data[:,:-1]\n",
    "    y = data[:,-1]\n",
    "    \n",
    "    return X.astype(np.int32), y.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mal, y_mal = load_malware()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_mal = StandardScaler().fit_transform(X_mal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_loan, y_loan = load_loan_prediction()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_loan = StandardScaler().fit_transform(X_loan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "X_risk, y_risk = load_credit_risk()\n",
    "X_fraud, y_fraud = load_credit_fraud()\n",
    "\n",
    "#Standardizing data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_risk = StandardScaler().fit_transform(X_risk)\n",
    "X_fraud = StandardScaler().fit_transform(X_fraud)\n",
    "\n",
    "#Filling nan values with mean using impute\n",
    "from sklearn.impute import SimpleImputer\n",
    "X_risk = SimpleImputer(strategy='mean').fit_transform(X_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validity for Credit Risk Dataset: {True}\n",
      "Validity for Credit Fraud Dataset: {True}\n"
     ]
    }
   ],
   "source": [
    "#Checking if data is valid\n",
    "def data_is_valid(X,y, examples, features):\n",
    "    return {\n",
    "        X.shape == (examples, features)\n",
    "        and y.shape == (examples,)\n",
    "        and not np.any(np.isnan(X))\n",
    "        and np.all((y==1) | (y==0))\n",
    "    }\n",
    "print(f'Validity for Credit Risk Dataset: {data_is_valid(X_risk, y_risk, 2000, 9)}')\n",
    "print(f'Validity for Credit Fraud Dataset: {data_is_valid(X_fraud, y_fraud, 10000, 29)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param grids for classifiers\n",
    "svc_param_grid = {'C': [0.1, 1, 10, 100], 'kernel': ['rbf']}\n",
    "kNN_param_grid = {'n_neighbors': [3,5,7,9]}\n",
    "forest_param_grid = {'n_estimators': [50, 100, 150]}\n",
    "\n",
    "#setting cv\n",
    "cv = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell for calculating base accuracies of the credit risk datatset\n",
    "\n",
    "#svc classifier\n",
    "classifier_1 = GridSearchCV(svm.SVC(), svc_param_grid)\n",
    "classifier_1.fit(X_risk, y_risk)\n",
    "risk_svc_accuracy = np.mean(cross_val_score(classifier_1, X_risk, y_risk, cv=cv, scoring='accuracy'))\n",
    "\n",
    "#kNN classifier\n",
    "classifier_2 = GridSearchCV(KNeighborsClassifier(), kNN_param_grid)\n",
    "classifier_2.fit(X_risk, y_risk)\n",
    "risk_kNN_accuracy = np.mean(cross_val_score(classifier_2, X_risk, y_risk, cv=cv, scoring='accuracy'))\n",
    "\n",
    "#random forest classifier\n",
    "classifier_3 = GridSearchCV(RandomForestClassifier(), forest_param_grid)\n",
    "classifier_3.fit(X_risk, y_risk)\n",
    "risk_forest_accuracy = np.mean(cross_val_score(classifier_3, X_risk, y_risk, cv=cv, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell for calculating base accuracies of the credit fraud datatset\n",
    "\n",
    "#svc classifier\n",
    "classifier_1 = GridSearchCV(svm.SVC(), svc_param_grid)\n",
    "classifier_1.fit(X_risk, y_risk)\n",
    "fraud_svc_accuracy = np.mean(cross_val_score(classifier_1, X_fraud, y_fraud, cv=cv, scoring='accuracy'))\n",
    "\n",
    "#kNN classifier\n",
    "classifier_2 = GridSearchCV(KNeighborsClassifier(), kNN_param_grid)\n",
    "classifier_2.fit(X_risk, y_risk)\n",
    "fraud_kNN_accuracy = np.mean(cross_val_score(classifier_2, X_fraud, y_fraud, cv=cv, scoring='accuracy'))\n",
    "\n",
    "#random forest classifier\n",
    "classifier_3 = GridSearchCV(RandomForestClassifier(), forest_param_grid)\n",
    "classifier_3.fit(X_risk, y_risk)\n",
    "fraud_forest_accuracy = np.mean(cross_val_score(classifier_3, X_fraud, y_fraud, cv=cv, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell for calculating base accuracies of the loan prediction datatset\n",
    "\n",
    "#svc classifier\n",
    "classifier_1 = GridSearchCV(svm.SVC(), svc_param_grid)\n",
    "classifier_1.fit(X_loan, y_loan)\n",
    "loan_svc_accuracy = np.mean(cross_val_score(classifier_1, X_loan, y_loan, cv=cv, scoring='accuracy'))\n",
    "\n",
    "#kNN classifier\n",
    "classifier_2 = GridSearchCV(KNeighborsClassifier(), kNN_param_grid)\n",
    "classifier_2.fit(X_loan, y_loan)\n",
    "loan_kNN_accuracy = np.mean(cross_val_score(classifier_2, X_loan, y_loan, cv=cv, scoring='accuracy'))\n",
    "\n",
    "#random forest classifier\n",
    "classifier_3 = GridSearchCV(RandomForestClassifier(), forest_param_grid)\n",
    "classifier_3.fit(X_loan, y_loan)\n",
    "loan_forest_accuracy = np.mean(cross_val_score(classifier_3, X_loan, y_loan, cv=cv, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell for calculating base accuracies of the malware datatset\n",
    "\n",
    "#svc classifier\n",
    "classifier_1 = GridSearchCV(svm.SVC(), svc_param_grid)\n",
    "classifier_1.fit(X_mal, y_mal)\n",
    "mal_svc_accuracy = np.mean(cross_val_score(classifier_1, X_mal, y_mal, cv=cv, scoring='accuracy'))\n",
    "\n",
    "#kNN classifier\n",
    "classifier_2 = GridSearchCV(KNeighborsClassifier(), kNN_param_grid)\n",
    "classifier_2.fit(X_mal, y_mal)\n",
    "mal_kNN_accuracy = np.mean(cross_val_score(classifier_2, X_mal, y_mal, cv=cv, scoring='accuracy'))\n",
    "\n",
    "#random forest classifier\n",
    "classifier_3 = GridSearchCV(RandomForestClassifier(), forest_param_grid)\n",
    "classifier_3.fit(X_mal, y_mal)\n",
    "mal_forest_accuracy = np.mean(cross_val_score(classifier_3, X_mal, y_mal, cv=cv, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base accuracies using three classifiers for both datasets:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVC</th>\n",
       "      <th>kNN</th>\n",
       "      <th>Random Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Credit Risk Dataset</th>\n",
       "      <td>0.5580</td>\n",
       "      <td>0.5395</td>\n",
       "      <td>0.5770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Credit Fraud Dataset</th>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         SVC     kNN  Random Forest\n",
       "Credit Risk Dataset   0.5580  0.5395         0.5770\n",
       "Credit Fraud Dataset  0.9913  0.9815         0.9848"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_accuracies = [\n",
    "    [risk_svc_accuracy, risk_kNN_accuracy, risk_forest_accuracy],\n",
    "    [fraud_svc_accuracy, fraud_kNN_accuracy, fraud_forest_accuracy]\n",
    "]\n",
    "print('Base accuracies using three classifiers for both datasets:')\n",
    "pd.DataFrame(base_accuracies, columns=['SVC', 'kNN', 'Random Forest'], index=['Credit Risk Dataset', 'Credit Fraud Dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVC</th>\n",
       "      <th>kNN</th>\n",
       "      <th>Random Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Loan Dataset</th>\n",
       "      <td>0.804167</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.795833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malware Dataset</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      SVC  kNN  Random Forest\n",
       "Loan Dataset     0.804167  0.8       0.795833\n",
       "Malware Dataset  1.000000  1.0       1.000000"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = [\n",
    "    [loan_svc_accuracy, loan_kNN_accuracy, loan_forest_accuracy],\n",
    "    [mal_svc_accuracy, mal_kNN_accuracy, mal_forest_accuracy]\n",
    "]\n",
    "pd.DataFrame(accuracies, columns=['SVC', 'kNN', 'Random Forest'], index=['Loan Dataset', 'Malware Dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries needed for GAN\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#generator and discriminator functions\n",
    "def build_generator(latent_dim, output_dim):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(128, input_dim=latent_dim, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(output_dim, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def build_discriminator(input_dim):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(128, input_dim=input_dim, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "def train_gan(generator, discriminator, gan, data, latent_dim, epochs=10000, batch_size=32):\n",
    "    generator.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train discriminator on real data\n",
    "        real_data = data[np.random.randint(0, data.shape[0], batch_size)]\n",
    "        real_labels = np.ones((batch_size, 1))\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
    "\n",
    "        # Train discriminator on generated data\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        generated_data = generator.predict(noise)\n",
    "        fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "        d_loss_fake = discriminator.train_on_batch(generated_data, fake_labels)\n",
    "\n",
    "        # Train generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        valid_labels = np.ones((batch_size, 1))\n",
    "\n",
    "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
    "def generate_synthetic_data(generator, latent_dim, n_samples):\n",
    "    noise = np.random.normal(0, 1, (n_samples, latent_dim))\n",
    "    generated_data = generator.predict(noise)\n",
    "    return generated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(X)\n",
    "\n",
    "latent_dim = 10  # Size of the random noise vector\n",
    "output_dim = 20  # Number of features\n",
    "\n",
    "generator = build_generator(latent_dim, output_dim)\n",
    "discriminator = build_discriminator(output_dim)\n",
    "gan = build_gan(generator, discriminator)\n",
    "\n",
    "epochs = 5000\n",
    "batch_size = 32\n",
    "\n",
    "train_gan(generator, discriminator, gan, scaled_data, latent_dim, epochs, batch_size)\n",
    "\n",
    "n_samples = 100\n",
    "generated_data = generate_synthetic_data(generator, latent_dim, n_samples)\n",
    "\n",
    "generated_data = scaler.inverse_transform(generated_data)\n",
    "\n",
    "print(\"Generated Data:\")\n",
    "print(generated_data[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
